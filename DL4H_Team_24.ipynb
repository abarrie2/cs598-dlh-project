{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abarrie2/cs598-dlh-project/blob/main/DL4H_Team_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01aH0PR4Sg-"
      },
      "source": [
        "# Grading rubric\n",
        "\n",
        "## Draft (20 Points)\n",
        "\n",
        "Jupyter Notebook (both .PDF and .ipynb files)\n",
        "\n",
        "You need to use the report template and fill out the following sections, each of which we will score based on the clarity and appropriateness of your writing (percentage of total grade for each component shown). All the information must be in the Jupyter notebook.\n",
        "\n",
        "- Introduction (2)\n",
        "  - A clear, high-level description of what the original paper is about and what is the contribution of it\n",
        "- Scope of reproducibility (2)\n",
        "- Methodology (8)\n",
        "  - Data\n",
        "    - Data descriptions\n",
        "    - Implementation code\n",
        "  - Model\n",
        "    - Model descriptions\n",
        "    - Implementation code\n",
        "  - Training\n",
        "    - Computational requirements\n",
        "    - Implementation code\n",
        "  - Evaluation\n",
        "    - Metrics descriptions\n",
        "    - Implementation code\n",
        "- Results (8)\n",
        "  - Results\n",
        "  - Analyses\n",
        "  - Plans\n",
        "\n",
        "## Final Descriptive Notebook Report (55 Points)\n",
        "\n",
        "Jupyter Notebook (both in .PDF and .ipynb format)\n",
        "\n",
        "- Introduction (5):\n",
        "    - A clear, high-level description of what the original paper is about and what is the contribution of it\n",
        "- Scope of reproducibility (5)\n",
        "- Methodology (15)\n",
        "  - Environment\n",
        "    - Python version\n",
        "    - Dependencies/packages needed\n",
        "  - Data\n",
        "    - Data download instruction\n",
        "    - Data descriptions with helpful charts and visualizations\n",
        "    - Preprocessing code + command\n",
        "  - Model\n",
        "    - Citation to the original paper\n",
        "    - Link to the original paper’s repo (if applicable)\n",
        "    - Model descriptions\n",
        "    - Implementation code\n",
        "    - Pretrained model (if applicable)\n",
        "  - Training\n",
        "    - Hyperparams\n",
        "      - Report at least 3 types of hyperparameters such as learning rate, batch size, hidden size, dropout\n",
        "    - Computational requirements\n",
        "      - Report at least 3 types of requirements such as type of hardware, average runtime for each epoch, total number of trials, GPU hrs used, # training epochs\n",
        "      - Training code\n",
        "  - Evaluation\n",
        "    - Metrics descriptions\n",
        "    - Evaluation code\n",
        "- Results (15)\n",
        "  - Table of results (no need to include additional experiments, but main reproducibility result should be included)\n",
        "  - All claims should be supported by experiment results\n",
        "  - Discuss with respect to the hypothesis and results from the original paper\n",
        "  - Experiments beyond the original paper\n",
        "    - Credits for each experiment depend on how hard it is to run the experiments. Each experiment should include results and a discussion\n",
        "    - Ablation Study.\n",
        "- Discussion (10)\n",
        "  - Implications of the experimental results, whether the original paper was reproducible, and if it wasn’t, what factors made it irreproducible\n",
        "  - “What was easy”\n",
        "  - “What was difficult”\n",
        "  - Recommendations to the original authors or others who work in this area for improving reproducibility\n",
        "- Public GitHub Repo (5)\n",
        "  - Publish your code in a public repository on GitHub and attach the URL in the notebook.\n",
        "  - Make sure your code is documented properly. \n",
        "    - A README.md file describing the exact steps to run your code is required. \n",
        "    - Check [ML Code Completeness Checklist](https://github.com/paperswithcode/releasing-research-code)\n",
        "    - Check [Best Practices for Reproducibility](https://www.cs.mcgill.ca/~ksinha4/practices_for_reproducibility/)\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "outputs": [],
      "source": [
        "# code comment is used as inline annotations for your coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: xxxxxxx\n",
        "2.   Hypothesis 2: xxxxxxx\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4WUjz64C3B"
      },
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(img_dir)\n",
        "cv2.imshow(\"Title\", img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create environment\n",
        "\n",
        "Create `conda` environment for the project using the `environment.yml` file:\n",
        "\n",
        "```bash\n",
        "conda env create --prefix .envs/dlh-team24 -f environment.yml\n",
        "```\n",
        "\n",
        "Activate the environment with:\n",
        "```bash\n",
        "conda activate .envs/dlh-team24\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import vitaldb\n",
        "\n",
        "#from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Local Data Caches\n",
        "\n",
        "Since the VitalDB data is static, local copies are stored and reused to avoid expensive downloads and to speed up data processing.\n",
        "\n",
        "The default directory defined below is already in the project `.gitignore` file. If later modified, it should also be added to the project `.gitignore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VITALDB_CACHE = './vitaldb_cache'\n",
        "VITAL_ALL = 'vital_all'\n",
        "VITAL_MINI = 'vital_mini'\n",
        "VITAL_METADATA = 'metadata'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p $VITALDB_CACHE\n",
        "!mkdir -p $VITALDB_CACHE/$VITAL_ALL\n",
        "!mkdir -p $VITALDB_CACHE/$VITAL_MINI\n",
        "!mkdir -p $VITALDB_CACHE/$VITAL_METADATA\n",
        "!ls -l $VITALDB_CACHE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OSFS Bulk Data Download\n",
        "\n",
        "**This step is not required, but will significantly speed up downstream processing and avoid a high volume of API requests to the VitalDB web site.**\n",
        "\n",
        "The cache population code checks if OSFS bulk download data of VitalDB vital files is locally available.\n",
        "\n",
        "- Manually downloaded the OSF Store archives from the following site: https://osf.io/dtc45/\n",
        "    - `Vital Files 0001-2000`\n",
        "    - `Vital Files 2001-4000`\n",
        "    - `Vital Files 4001-6388`\n",
        "- Once the `OSF Storage (United States)` link is clicked a `Download as zip` link will appear.\n",
        "- Once downloaded, extract each of the 3 zip archives.\n",
        "- Move all files from each of the unzip directories into the `${VITALDB_CACHE}/${VITAL_ALL}` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Returns the Pandas DataFrame for the specified dataset.\n",
        "#   One of 'cases', 'labs', or 'trks'\n",
        "# If the file exists locally, create and return the DataFrame.\n",
        "# Else, download and cache the csv first, then return the DataFrame.\n",
        "def vitaldb_dataframe_loader(dataset_name):\n",
        "    if dataset_name not in ['cases', 'labs', 'trks']:\n",
        "        raise ValueError(f'Invalid dataset name: {dataset_name}')\n",
        "    file_path = f'{VITALDB_CACHE}/{VITAL_METADATA}/{dataset_name}.csv'\n",
        "    if os.path.isfile(file_path):\n",
        "        print(f'{dataset_name}.csv exists locally.')\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df\n",
        "    else:\n",
        "        print(f'downloading {dataset_name} and storing in the local cache for future reuse.')\n",
        "        df = pd.read_csv(f'https://api.vitaldb.net/{dataset_name}')\n",
        "        df.to_csv(file_path, index=False)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cases = vitaldb_dataframe_loader('cases')\n",
        "cases = cases.set_index('caseid')\n",
        "cases.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cases.index.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cases.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cases['sex'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks = vitaldb_dataframe_loader('trks')\n",
        "trks = trks.set_index('caseid')\n",
        "trks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks.index.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks.groupby('caseid')[['tid']].count().plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks.groupby('caseid')[['tid']].count().hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks.groupby('tname').count().sort_values(by='tid', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters of Interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hemodynamic Parameters Reference\n",
        "https://vitaldb.net/dataset/?query=overview#h.f7d712ycdpk2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solar8000/ART_MBP**\n",
        "\n",
        "mean blood pressure\n",
        "\n",
        "Parameter, Description, Type/Hz, Unit\n",
        "\n",
        "Solar8000/ART_MBP, Mean arterial pressure, N, mmHg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks[trks['tname'].str.contains('Solar8000/ART_MBP')].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SNUADC/ART**\n",
        "\n",
        "arterial blood pressure waveform\n",
        "\n",
        "Parameter, Description, Type/Hz, Unit\n",
        "\n",
        "SNUADC/ART, Arterial pressure wave, W/500, mmHg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks[trks['tname'].str.contains('SNUADC/ART')].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SNUADC/ECG_II**\n",
        "\n",
        "electrocardiogram waveform\n",
        "\n",
        "Parameter, Description, Type/Hz, Unit\n",
        "\n",
        "SNUADC/ECG_II, ECG lead II wave, W/500, mV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks[trks['tname'].str.contains('SNUADC/ECG_II')].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**BIS/EEG1_WAV**\n",
        "\n",
        "electroencephalogram waveform\n",
        "\n",
        "Parameter, Description, Type/Hz, Unit\n",
        "\n",
        "BIS/EEG1_WAV, EEG wave from channel 1, W/128, uV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks[trks['tname'].str.contains('BIS/EEG1_WAV')].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cases of Interest\n",
        "\n",
        "These are the subset of case ids for which modelling and analysis will be performed based upon inclusion criteria and waveform data availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRACK_NAMES = ['SNUADC/ART', 'SNUADC/ECG_II', 'BIS/EEG1_WAV']\n",
        "TRACK_SRATES = [500, 500, 128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As in the paper, select cases which meet the following criteria:\n",
        "#\n",
        "# For patients, the inclusion criteria were as follows:\n",
        "# (1) adults (age >= 18)\n",
        "# (2) administered general anaesthesia\n",
        "# (3) undergone non-cardiac surgery. \n",
        "#\n",
        "# For waveform data, the inclusion criteria were as follows:\n",
        "# (1) no missing monitoring for ABP, ECG, and EEG waveforms\n",
        "# (2) no cases containing false events or non-events due to poor signal quality\n",
        "#     (checked in second stage of data preprocessing)\n",
        "\n",
        "# adult\n",
        "inclusion_1 = cases.loc[cases['age'] >= 18].index\n",
        "print(f'{len(cases)-len(inclusion_1)} cases excluded, {len(inclusion_1)} remaining due to age criteria')\n",
        "\n",
        "# general anesthesia\n",
        "inclusion_2 = cases.loc[cases['ane_type'] == 'General'].index\n",
        "print(f'{len(cases)-len(inclusion_2)} cases excluded, {len(inclusion_2)} remaining due to anesthesia criteria')\n",
        "\n",
        "# non-cardiac surgery\n",
        "inclusion_3 = cases.loc[\n",
        "    ~cases['opname'].str.contains(\"cardiac\", case=False)\n",
        "    & ~cases['opname'].str.contains(\"aneurysmal\", case=False)\n",
        "].index\n",
        "print(f'{len(cases)-len(inclusion_3)} cases excluded, {len(inclusion_3)} remaining due to non-cardiac surgery criteria')\n",
        "\n",
        "# ABP, ECG, EEG waveforms\n",
        "inclusion_4 = trks.loc[trks['tname'].isin(TRACK_NAMES)].index.value_counts()\n",
        "inclusion_4 = inclusion_4[inclusion_4 == len(TRACK_NAMES)].index\n",
        "print(f'{len(cases)-len(inclusion_4)} cases excluded, {len(inclusion_4)} remaining due to missing waveform data')\n",
        "\n",
        "cases_of_interest_idx = inclusion_1 \\\n",
        "    .intersection(inclusion_2) \\\n",
        "    .intersection(inclusion_3) \\\n",
        "    .intersection(inclusion_4)\n",
        "\n",
        "cases_of_interest = cases.loc[cases_of_interest_idx]\n",
        "\n",
        "print()\n",
        "print(f'{cases_of_interest_idx.shape[0]} out of {cases.shape[0]} total cases remaining after exclusions applied')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cases_of_interest.head(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tracks of Interest\n",
        "\n",
        "These are the subset of tracks (waveforms) for the cases of interest identified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A single case maps to one or more waveform tracks. Select only the tracks required for analysis.\n",
        "trks_of_interest = trks.loc[cases_of_interest_idx][trks.loc[cases_of_interest_idx]['tname'].isin(TRACK_NAMES)]\n",
        "trks_of_interest.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks_of_interest.head(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trks_of_interest_idx = trks_of_interest.set_index('tid').index\n",
        "trks_of_interest_idx.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Tracks Cache for Local Processing\n",
        "\n",
        "Tracks data are large and therefore expensive to download every time used.\n",
        "By default, the vital file format stores all tracks for each case internally. Since only certain tracks per case are required, each vital file can be further truncated to only store the tracks for needed waveforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Maximum number of cases of interest for which to download data.\n",
        "# Set to a small value for demo purposes, else set to None to disable and download all.\n",
        "#MAX_CASES = None\n",
        "MAX_CASES = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trim cases of interest to MAX_CASES\n",
        "if MAX_CASES:\n",
        "    cases_of_interest_idx = cases_of_interest_idx[:MAX_CASES]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure the full vital file dataset is available for cases of interest.\n",
        "count_downloaded = 0\n",
        "count_present = 0\n",
        "\n",
        "#for i, idx in enumerate(cases.index):\n",
        "for i, idx in enumerate(cases_of_interest_idx):\n",
        "    if MAX_CASES and i >= MAX_CASES:\n",
        "        break\n",
        "\n",
        "    full_path = f'{VITALDB_CACHE}/{VITAL_ALL}/{idx:04d}.vital'\n",
        "    if not os.path.isfile(full_path):\n",
        "        print(f'Missing vital file: {full_path}')\n",
        "        # Download and save the file.\n",
        "        vf = vitaldb.VitalFile(idx)\n",
        "        vf.to_vital(full_path)\n",
        "        count_downloaded += 1\n",
        "    else:\n",
        "        count_present += 1\n",
        "\n",
        "print()\n",
        "print(f'Count of cases of interest:           {cases_of_interest_idx.shape[0]}')\n",
        "print(f'Count of vital files downloaded:      {count_downloaded}')\n",
        "print(f'Count of vital files already present: {count_present}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert vital files to \"mini\" versions including only the subset of tracks based on TRACK_NAMES defined above.\n",
        "# Only perform conversion for the cases of interest.\n",
        "# NOTE: If this cell is interrupted, it can be restarted and will continue where it left off.\n",
        "count_minified = 0\n",
        "count_present = 0\n",
        "\n",
        "for i, idx in enumerate(cases_of_interest_idx):\n",
        "    if MAX_CASES and i >= MAX_CASES:\n",
        "        break\n",
        "    \n",
        "    full_path = f'{VITALDB_CACHE}/{VITAL_ALL}/{idx:04d}.vital'\n",
        "    mini_path = f'{VITALDB_CACHE}/{VITAL_MINI}/{idx:04d}_mini.vital'\n",
        "    if not os.path.isfile(mini_path):\n",
        "        print(f'Creating mini vital file: {idx}')\n",
        "        vf = vitaldb.VitalFile(full_path, TRACK_NAMES)\n",
        "        vf.to_vital(mini_path)\n",
        "        count_minified += 1\n",
        "    else:\n",
        "        count_present += 1\n",
        "\n",
        "print()\n",
        "print(f'Count of cases of interest:           {cases_of_interest_idx.shape[0]}')\n",
        "print(f'Count of vital files minified:        {count_minified}')\n",
        "print(f'Count of vital files already present: {count_present}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exclude cases where ABP j signal quality (jSQI) < 0.8\n",
        "# TODO: Implement jSQI function\n",
        "# TODO: Filter cases with jSQI < 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate hypotensive events\n",
        "# Hypotensive events are defined as a 1-minute interval with sustained ABP of less than 65 mmHg\n",
        "# Note: Hypotensive events should be at least 20 minutes apart to minimize potential residual effects from previous events\n",
        "# TODO: Implement hypotension event generation function\n",
        "# TODO: Generate hypotension events\n",
        "\n",
        "# Generate hypotension non-events\n",
        "# To sample non-events, 30-minute segments where the ABP was above 75 mmHG were selected, and then\n",
        "# three one-minute samples of each waveform were obtained from the middle of the segment\n",
        "# TODO: Implement hypotension non-event generation function\n",
        "# TODO: Generate hypotension non-events\n",
        "\n",
        "# XXX Create dummy events with random labels for now\n",
        "def generate_dummy_data(cases_of_interest_idx):\n",
        "    # Initialize an empty DataFrame\n",
        "    generated_data = []\n",
        "    \n",
        "    # Loop through each case index\n",
        "    for case in cases_of_interest_idx:\n",
        "        # Generate a random number of rows between 5 and 20\n",
        "        num_rows = random.randint(5, 20)\n",
        "        \n",
        "        # Generate data for each row\n",
        "        for _ in range(num_rows):\n",
        "            starttime = random.randint(0, 1200)\n",
        "            endtime = starttime + 60\n",
        "            label = random.randint(0, 1)\n",
        "            \n",
        "            # Append the data to the DataFrame\n",
        "            generated_data.append([\n",
        "                case,\n",
        "                starttime,\n",
        "                endtime,\n",
        "                label\n",
        "            ])\n",
        "    \n",
        "    return pd.DataFrame(generated_data, columns=['caseidx', 'starttime', 'endtime', 'label'])\n",
        "samples = generate_dummy_data(cases_of_interest_idx)\n",
        "samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data tracks\n",
        "\n",
        "# ABP waveforms are used without further pre-processing\n",
        "# ECG waveforms are band-pass filtered between 1 and 40 Hz, and Z-score normalized\n",
        "# EEG waveforms are band-pass filtered between 0.5 and 40 Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training, validation, and test sets\n",
        "# Use 6:1:3 ratio and prevent samples from a single case from being split across different sets\n",
        "# Note: number of samples at each time point is not the same, because the first event can occur before the 3/5/10/15 minute mark\n",
        "\n",
        "# Set target sizes\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.1\n",
        "test_ratio = 1 - train_ratio - val_ratio # ensure ratios sum to 1\n",
        "\n",
        "# Assume that on average cases have the ~same number of events so we can split by case rather than event\n",
        "# Note: this means that the ratios will be approximate\n",
        "\n",
        "# Get unique cases\n",
        "unique_cases = samples['caseidx'].unique()\n",
        "\n",
        "# Split cases into train and other\n",
        "train_caseidx, other_caseidx = train_test_split(unique_cases, test_size=(1 - train_ratio), random_state=42)\n",
        "# Split other into val and test\n",
        "val_caseidx, test_caseidx = train_test_split(other_caseidx, test_size=(test_ratio / (1 - train_ratio)), random_state=42)\n",
        "\n",
        "# Create datasets\n",
        "samples_train = samples[samples['caseidx'].isin(train_caseidx)]\n",
        "samples_val = samples[samples['caseidx'].isin(val_caseidx)]\n",
        "samples_test = samples[samples['caseidx'].isin(test_caseidx)]\n",
        "\n",
        "# Check how many samples are in each set\n",
        "print(f\"Train samples: {len(samples_train)}, ({len(samples_train) / len(samples):.2%})\")\n",
        "print(f\"Val samples: {len(samples_val)}, ({len(samples_val) / len(samples):.2%})\")\n",
        "print(f\"Test samples: {len(samples_test)}, ({len(samples_test) / len(samples):.2%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create vitalDataset class\n",
        "class vitalDataset(Dataset):\n",
        "    def __init__(self, file_dir, samples, track_names, track_srates_hz):\n",
        "        # samples should be a list of (caseidx, starttime, endtime, label)\n",
        "        self.file_dir = file_dir\n",
        "        self.samples = samples\n",
        "        self.track_names = track_names\n",
        "        self.track_srates_hz = track_srates_hz\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get metadata for this event\n",
        "        caseidx, starttime, endtime, label = self.samples.iloc[idx]\n",
        "        # Load vital file\n",
        "        file_path = os.path.join(self.file_dir, f\"{caseidx:04d}.vital\")\n",
        "        vf = vitaldb.VitalFile(file_path, self.track_names)\n",
        "        # Crop samples to target interval\n",
        "        vf.crop(starttime, endtime)\n",
        "        # Create target tensor\n",
        "        samples = torch.zeros(len(self.track_names)*int((endtime-starttime)*sum(self.track_srates_hz)))\n",
        "        # Populate each track\n",
        "        for i, (track_name, rate) in enumerate(zip(self.track_names, self.track_srates_hz)):\n",
        "            # Get samples for this track\n",
        "            track_samples, _ = vf.get_samples(track_name, 1/rate)\n",
        "            #track_samples = vf.to_numpy(track_name, 1/rate)\n",
        "            # Convert to tensor and store in samples\n",
        "            start = int((endtime-starttime)*sum(self.track_srates_hz[:i]))\n",
        "            end = start + int((endtime-starttime)*self.track_srates_hz[i])\n",
        "            samples[start:end] = torch.tensor(track_samples)\n",
        "        return samples, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = vitalDataset(f'{VITALDB_CACHE}/{VITAL_ALL}/', samples_train, TRACK_NAMES, TRACK_SRATES)\n",
        "val_dataset = vitalDataset(f'{VITALDB_CACHE}/{VITAL_ALL}/', samples_val, TRACK_NAMES, TRACK_SRATES)\n",
        "test_dataset = vitalDataset(f'{VITALDB_CACHE}/{VITAL_ALL}/', samples_test, TRACK_NAMES, TRACK_SRATES)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  return None\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  return None\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "  return None\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "outputs": [],
      "source": [
        "class my_model():\n",
        "  # use this class to define your model\n",
        "  pass\n",
        "\n",
        "model = my_model()\n",
        "loss_func = None\n",
        "optimizer = None\n",
        "\n",
        "def train_model_one_iter(model, loss_func, optimizer):\n",
        "  pass\n",
        "\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "for i in range(num_epoch):\n",
        "  train_model_one_iter(model, loss_func, optimizer)\n",
        "  train_loss, valid_loss = None, None\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "outputs": [],
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmVuzQ724HbO"
      },
      "source": [
        "# Feel free to add new sections"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
